# -*- coding: utf-8 -*-
"""cartpole.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cS9MtdTqJlC02KDqZRwMWZKPop1EaeXQ
"""

import copy
from copy import deepcopy
import numpy as np
import gym
import random

class VectorizedEnvWrapper(gym.Wrapper):
    def __init__(self, env, num_envs=1):
        '''
        env (gym.Env): to make copies of
        num_envs (int): number of copies
        '''
        super().__init__(env)
        self.num_envs = num_envs
        self.envs = [copy.deepcopy(env) for n in range(num_envs)]

    def reset(self):
        '''
        Return and reset each environment
        '''
        return np.asarray([env.reset() for env in self.envs])

    def step(self, actions):
        '''
        Take a step in the environment and return the result.
        actions (torch.tensor)
        '''
        next_states, rewards, dones = [], [], []
        for env, action in zip(self.envs, actions):
            next_state, reward, done, _ = env.step(action.item())
            if done:
                next_states.append(env.reset())
            else:
                next_states.append(next_state)
            rewards.append(reward)
            dones.append(done)
        return np.asarray(next_states), np.asarray(rewards), \
            np.asarray(dones)

import torch

class Policy:
    def pi(self, s_t):
        '''
        returns the probability distribution over actions 
        (torch.distributions.Distribution)
        
        s_t (np.ndarray): the current state
        '''
        raise NotImplementedError
    
    def act(self, s_t):
        '''
        s_t (np.ndarray): the current state
        Because of environment vectorization, this will produce
        E actions where E is the number of parallel environments.
        '''
        a_t = self.pi(s_t).sample()
        return a_t
    
    def learn(self, states, actions, returns):
        '''
        states (np.ndarray): the list of states encountered during
                             rollout
        actions (np.ndarray): the list of actions encountered during
                              rollout
        returns (np.ndarray): the list of returns encountered during
                              rollout
        
        Because of environment vectorization, each of these has first
        two dimensions TxE where T is the number of time steps in the
        rollout and E is the number of parallel environments.
        '''
        actions = torch.tensor(actions)
        returns = torch.tensor(returns)

        log_prob = self.pi(states).log_prob(actions)
        loss = torch.mean(-log_prob*returns)
        self.opt.zero_grad()
        # loss.backward(retain_graph=True)
        loss.backward()
        # for key in self.p.state_dict():
        #     print(key, self.p.state_dict()[key])
        # print(self.p[0].weight.grad)
        self.opt.step()
        # return(self.p.state_dict())

class DiagonalGaussianPolicy(Policy):
    def __init__(self, env, lr=1e-2):
        '''
        env (gym.Env): the environment
        lr (float): learning rate
        '''
        self.N = env.observation_space.shape[0]
        self.M = env.action_space.shape[0]
        # self.mu = torch.nn.Sequential(
        #     torch.nn.Linear(self.N, self.M),
        # ).double()
        self.mu = torch.nn.Sequential(
            torch.nn.Linear(self.N, 64),
            torch.nn.ReLU(),
            torch.nn.Linear(64, 64),
            torch.nn.ReLU(),
            torch.nn.Linear(64, self.M)
        ).double()

        self.log_sigma = torch.ones(self.M, dtype=torch.double, requires_grad=True)

        self.opt = torch.optim.Adam(list(self.mu.parameters()) + [self.log_sigma], lr=lr)
        
    def pi(self, s_t):
        '''
        returns the probability distribution over actions
        s_t (np.ndarray): the current state
        '''
        s_t = torch.as_tensor(s_t).double()
        mu = self.mu(s_t)
        log_sigma = self.log_sigma
        sigma = torch.exp(log_sigma)
        pi = torch.distributions.MultivariateNormal(mu, torch.diag(sigma))
        return pi

def calculate_returns(rewards, dones, gamma):
    result = np.empty_like(rewards)
    result[-1] = rewards[-1]
    for t in range(len(rewards)-2, -1, -1):
        result[t] = rewards[t] + gamma*(1-dones[t])*result[t+1]
    return result

import seaborn as sns; sns.set()

def REINFORCE_client(env, agent, gamma=0.99, T=1000):    
    # for learning    
    states = np.empty((T, env.num_envs, agent.N))
    if isinstance(env.action_space, gym.spaces.Discrete):
        # discrete action spaces only need to store a 
        # scalar for each action.
        actions = np.empty((T, env.num_envs))
    else:
        # continuous action spaces need to store a 
        # vector for each eaction.
        actions = np.empty((T, env.num_envs, agent.M))
    rewards = np.empty((T, env.num_envs))
    dones = np.empty((T, env.num_envs))
    
    # for plotting
    s_t = env.reset()

    for t in range(T):
        a_t = agent.act(s_t)
        s_t_next, r_t, d_t = env.step(a_t)

        # for learning
        states[t] = s_t
        actions[t] = a_t
        rewards[t] = r_t
        dones[t] = d_t

        s_t = s_t_next

    returns = calculate_returns(rewards, dones, gamma)
    agent.learn(states, actions, returns)

    
            
    # return(agent.p.state_dict(), rewards.sum()/dones.sum())
    # return(list(agent.p.parameters()), rewards.sum()/dones.sum())
    return(list(agent.mu.parameters()), agent.log_sigma, rewards.sum()/dones.sum())

# from geom_median.torch import compute_geometric_median

import gym_cartpole_swingup

env = VectorizedEnvWrapper(gym.make("CartPoleSwingUp-v0"), num_envs=256)
N = env.observation_space.shape[0]
M = env.action_space.shape[0]
global_model = torch.nn.Sequential(
            torch.nn.Linear(N, 64),
            torch.nn.ReLU(),
            torch.nn.Linear(64, 64),
            torch.nn.ReLU(),
            torch.nn.Linear(64, M)
        ).double()
global_sigma = torch.ones(M, dtype=torch.double, requires_grad=True)

global_dict = global_model.state_dict()
Number=len(global_dict.keys())
client_models = []
for clients in range(10):
    client_models.append(DiagonalGaussianPolicy(env, lr=1e-2))

for model in client_models:
    model.mu.load_state_dict(global_model.state_dict())

totals = []
for epoch in range(20):
  grads = []
  sigma_collected = []
  rewards_collected = []
  for clients in range(10):
    g, s, r = REINFORCE_client(env, client_models[clients], gamma=0.999)
    grads.append(g)
    sigma_collected.append(s)
    rewards_collected.append(r)

  # Attack 
  # x=random.sample(range(0, 9), 2)
  # for i in range(2):
  #   sigma_collected[x[i]] = 100*torch.rand(sigma_collected[x[i]].shape)
  #   for j in range(Number):
  #     grads[x[i]][j] = 100*torch.rand(grads[x[i]][j].shape)


  j=0
  for k in global_dict.keys():
      global_dict[k] = torch.stack([grads[i][j].double() for i in range(10)], 0).mean(0)
      j=j+1
  global_model.load_state_dict(global_dict)

  with torch.no_grad():
    xs = torch.stack(sigma_collected)
    sigma_out = torch.mean(xs, 0)
  sigma_out.requires_grad_()
  for model in client_models:
        model.mu.load_state_dict(global_model.state_dict())
        model.log_sigma = sigma_out
        model.opt = torch.optim.Adam(list(model.mu.parameters()) + [model.log_sigma], lr=1e-2)

  for key in global_model.state_dict():
        print(key, global_model.state_dict()[key])

  print(sigma_out)
      

  totals.append(sum(rewards_collected)/10)


fig = sns.lineplot(x=range(len(totals)), y=totals)
fig_1 = fig.get_figure()
fig_1.savefig("out_fed.png")
# sns.lineplot(x=range(len(totals)), y=totals)

# agent.p.state_dict()
# agent = CategoricalPolicy(env, lr=1e-1)
# agent = REINFORCE(env, agent)